{
  "date": "2025-08-25T23:39:50.473948",
  "dataset": "gsm8k",
  "configuration": {
    "full_training": "Half",
    "epochs": 1,
    "batch_size": 4,
    "grad_accumulation": 4,
    "effective_batch_size": 16,
    "train_samples": 1000,
    "val_samples": 200
  },
  "final_accuracy_hard": 0.0,
  "final_accuracy_perplexity": 0.1,
  "best_accuracy_hard": 0.05,
  "best_accuracy_perplexity": 0.1,
  "winner_final": "Perplexity-based rewards",
  "winner_best": "Perplexity-based-based rewards",
  "improvement_percentage": 10.0,
  "recommendation": "Use perplexity-based reward: achieved best accuracy of 0.100. Perplexity rewards show smoother optimization.",
  "hard_metrics": [
    {
      "step": 100,
      "loss": 0.0,
      "reward_type": "hard",
      "learning_rate": 6.048171459482286e-05,
      "accuracy": 0.05,
      "perplexity": 2.827380895614624
    },
    {
      "step": 200,
      "loss": 0.0,
      "reward_type": "hard",
      "learning_rate": 9.720197773929749e-06,
      "accuracy": 0.05,
      "perplexity": 2.5987729489803315
    },
    {
      "step": 250,
      "loss": 1.0917689514160156,
      "reward_type": "hard",
      "learning_rate": 0.0,
      "accuracy": 0.0,
      "perplexity": 2.6738731145858763
    }
  ],
  "perplexity_metrics": [
    {
      "step": 100,
      "loss": 0.0,
      "reward_type": "perplexity",
      "learning_rate": 1.0174313404673378e-05,
      "accuracy": 0.0,
      "perplexity": 2.4666835844516752
    },
    {
      "step": 125,
      "loss": 1.133330795288086,
      "reward_type": "perplexity",
      "learning_rate": 0.0,
      "accuracy": 0.1,
      "perplexity": 2.5440863013267516
    }
  ]
}